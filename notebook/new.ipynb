{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ky/3k7xgnnd3cs47n38b42hqwbm0000gn/T/ipykernel_44439/3578963337.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Timestamp', 'state', 'self_employed', 'work_interfere', 'Country'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m data_file \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path)\n\u001b[1;32m     18\u001b[0m dropped_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomments\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mself_employed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwork_interfere\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCountry\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 19\u001b[0m \u001b[43mdata_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdropped_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m data_file\u001b[38;5;241m.\u001b[39mdropna(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/health_predictor/lib/python3.12/site-packages/pandas/core/frame.py:5568\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5421\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5422\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5429\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5430\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5431\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5432\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5433\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5566\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5567\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5570\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5574\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5575\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5576\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/health_predictor/lib/python3.12/site-packages/pandas/core/generic.py:4782\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4781\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4782\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/anaconda3/envs/health_predictor/lib/python3.12/site-packages/pandas/core/generic.py:4824\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4822\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4823\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4824\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4825\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4827\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4828\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/health_predictor/lib/python3.12/site-packages/pandas/core/indexes/base.py:7069\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   7068\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7069\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7070\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7071\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Timestamp', 'state', 'self_employed', 'work_interfere', 'Country'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import pickle\n",
    "\n",
    "path = './survey.csv'\n",
    "data_file = pd.read_csv(path)\n",
    "\n",
    "dropped_columns = ['comments', 'Timestamp', 'state', 'self_employed', 'work_interfere', 'Country']\n",
    "data_file.drop(dropped_columns, axis=1, inplace=True)\n",
    "data_file.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './survey.csv'\n",
    "data_file = pd.read_csv(path)\n",
    "data_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = data_file.drop(['comments', 'Timestamp', 'state', 'self_employed', 'work_interfere', 'Country'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_columns = ['comments', 'Timestamp', 'state', 'self_employed', 'work_interfere', 'Country']\n",
    "# data_file.drop(dropped_columns, axis=1, inplace=True)\n",
    "data_file.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process gender column\n",
    "def process_gender(gender):\n",
    "    gender = gender.lower()\n",
    "    if 'female' not in gender and ('male' in gender or gender == 'm'):\n",
    "        return 'male'\n",
    "    elif 'female' in gender or gender == 'f':\n",
    "        return 'female'\n",
    "    else:\n",
    "        return 'others'\n",
    "\n",
    "data_file['Gender'] = data_file['Gender'].apply(process_gender)\n",
    "\n",
    "# Filter age column\n",
    "data_file = data_file[(data_file['Age'] > 16) & (data_file['Age'] < 100)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "y = data_file['treatment'].replace(['Yes', 'No'], [1, 0])\n",
    "X = data_file.drop(columns=['treatment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numeric and categorical features\n",
    "numeric_features = ['Age']\n",
    "categorical_features = ['Gender', 'family_history', 'no_employees',\n",
    "                        'remote_work', 'tech_company', 'benefits', 'care_options',\n",
    "                        'wellness_program', 'seek_help', 'anonymity', 'leave',\n",
    "                        'mental_health_consequence', 'phys_health_consequence', 'coworkers',\n",
    "                        'supervisor', 'mental_health_interview', 'phys_health_interview',\n",
    "                        'mental_vs_physical', 'obs_consequence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)  # drop='first' to handle multicollinearity\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Get the feature names after one-hot encoding\n",
    "feature_names = numeric_features + list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features))\n",
    "\n",
    "# Convert the transformed data back to a DataFrame\n",
    "X_preprocessed_df = pd.DataFrame(X_preprocessed, columns=feature_names)\n",
    "\n",
    "# Display the first few rows of the preprocessed DataFrame\n",
    "print(X_preprocessed_df.head())\n",
    "\n",
    "#save the preprocessor\n",
    "with open('preprocessor.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessor, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and parameter grids\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    'MLPClassifier': MLPClassifier(),\n",
    "    'SVC': SVC()\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [None, 10, 20, 30],\n",
    "        'classifier__min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 1]\n",
    "    },\n",
    "    'MLPClassifier': {\n",
    "        'classifier__hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "        'classifier__activation': ['tanh', 'relu'],\n",
    "        'classifier__solver': ['sgd', 'adam'],\n",
    "        'classifier__alpha': [0.0001, 0.05],\n",
    "        'classifier__learning_rate': ['constant','adaptive']\n",
    "    },\n",
    "    'SVC': {\n",
    "        'classifier__C': [0.1, 1, 10, 100],\n",
    "        'classifier__gamma': [1, 0.1, 0.01, 0.001],\n",
    "        'classifier__kernel': ['rbf', 'poly', 'sigmoid']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# best_models = {}\n",
    "\n",
    "# for model_name, model in models.items():\n",
    "#     pipeline = Pipeline(steps=[\n",
    "#         ('preprocessor', preprocessor),\n",
    "#         ('classifier', model)\n",
    "#     ])\n",
    "    \n",
    "#     grid_search = GridSearchCV(pipeline, param_grids[model_name], cv=5, verbose=3, n_jobs=-1)\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "    \n",
    "#     best_models[model_name] = grid_search.best_estimator_\n",
    "    \n",
    "#     # Save the model with the highest accuracy\n",
    "#     if model_name == max(best_models, key=lambda k: grid_search.best_score_):\n",
    "#         with open(f'{model_name}_best_model.pkl', 'wb') as f:\n",
    "#             pickle.dump(best_models[model_name], f)\n",
    "\n",
    "#     print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "#     print(f\"Best cross-validation accuracy for {model_name}: {grid_search.best_score_}\\n\")\n",
    "\n",
    "# # Optionally, load the best model later\n",
    "# # with open(f'{model_name}_best_model.pkl', 'rb') as f:\n",
    "# #     loaded_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in best_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f'{model_name} results:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_rf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "pipeline_ab = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "pipeline_mlp = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', MLPClassifier())\n",
    "])\n",
    "\n",
    "pipeline_svc = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the VotingClassifier with pipelines\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', pipeline_rf),\n",
    "        ('ab', pipeline_ab),\n",
    "        ('mlp', pipeline_mlp),\n",
    "        ('svc', pipeline_svc)\n",
    "    ],\n",
    "    voting='hard'  # Use 'soft' for probability voting\n",
    ")\n",
    "\n",
    "# Fit the VotingClassifier\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the VotingClassifier\n",
    "y_pred_voting = voting_clf.predict(X_test)\n",
    "print(\"Ensemble Voting Classifier:\")\n",
    "print(f\"Classification report:\\n{classification_report(y_test, y_pred_voting)}\")\n",
    "print(f\"Confusion matrix:\\n{confusion_matrix(y_test, y_pred_voting)}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_voting)}\\n\")\n",
    "\n",
    "# Save the VotingClassifier\n",
    "with open('voting_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(voting_clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# Define the meta-model (final estimator)\n",
    "meta_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create the StackingClassifier with pipelines\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', pipeline_rf),\n",
    "        ('ab', pipeline_ab),\n",
    "        ('mlp', pipeline_mlp),\n",
    "        ('svc', pipeline_svc)\n",
    "    ],\n",
    "    final_estimator=meta_model,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Fit the StackingClassifier\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the StackingClassifier\n",
    "y_pred_stacking = stacking_clf.predict(X_test)\n",
    "print(\"Stacking Classifier:\")\n",
    "print(f\"Classification report:\\n{classification_report(y_test, y_pred_stacking)}\")\n",
    "print(f\"Confusion matrix:\\n{confusion_matrix(y_test, y_pred_stacking)}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_stacking)}\\n\")\n",
    "\n",
    "# Save the StackingClassifier\n",
    "with open('stacking_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(stacking_clf, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_stacking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "health_predictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
